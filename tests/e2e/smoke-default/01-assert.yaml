apiVersion: otel.splunk.com/v1alpha1
kind: Agent
metadata:
  labels:
    app.kubernetes.io/managed-by: splunk-otel-collector-operator
spec:
  clusterName: test-cluster
  realm: my-splunk-realm
---
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: splunk-otel-collector
    app.kubernetes.io/managed-by: splunk-otel-collector-operator
    app.kubernetes.io/name: test-default-agent
    app.kubernetes.io/part-of: opentelemetry
  name: test-default-agent
  namespace: kuttl-test-magical-man
data:
  relay.yaml: |
    exporters:
      sapm:
        access_token: ${SPLUNK_OBSERVABILITY_ACCESS_TOKEN}
        endpoint: https://ingest.${SPLUNK_REALM}.signalfx.com/v2/trace
      signalfx:
        access_token: ${SPLUNK_OBSERVABILITY_ACCESS_TOKEN}
        api_url: https://api.${SPLUNK_REALM}.signalfx.com
        correlation: null
        ingest_url: https://ingest.${SPLUNK_REALM}.signalfx.com
        sync_host_metadata: true
      splunk_hec/o11y:
        endpoint: https://ingest.${SPLUNK_REALM}.signalfx.com/v1/log
        log_data_enabled: true
        profiling_data_enabled: false
        token: ${SPLUNK_OBSERVABILITY_ACCESS_TOKEN}
    extensions:
      file_storage:
        directory: /var/addon/splunk/otel_pos
      health_check: null
      k8s_observer:
        auth_type: serviceAccount
        node: ${K8S_NODE_NAME}
      memory_ballast:
        size_mib: ${SPLUNK_BALLAST_SIZE_MIB}
      zpages: null
    processors:
      batch: null
      filter/logs:
        logs:
          exclude:
            match_type: strict
            resource_attributes:
              - key: splunk.com/exclude
                value: "true"
      k8sattributes:
        extract:
          annotations:
            - from: pod
              key: splunk.com/sourcetype
            - from: namespace
              key: splunk.com/exclude
              tag_name: splunk.com/exclude
            - from: pod
              key: splunk.com/exclude
              tag_name: splunk.com/exclude
            - from: namespace
              key: splunk.com/index
              tag_name: com.splunk.index
            - from: pod
              key: splunk.com/index
              tag_name: com.splunk.index
          labels:
            - key: app
          metadata:
            - k8s.namespace.name
            - k8s.node.name
            - k8s.pod.name
            - k8s.pod.uid
            - container.id
            - container.image.name
            - container.image.tag
        filter:
          node_from_env_var: K8S_NODE_NAME
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.uid
          - sources:
              - from: resource_attribute
                name: k8s.pod.ip
          - sources:
              - from: resource_attribute
                name: ip
          - sources:
              - from: connection
          - sources:
              - from: resource_attribute
                name: host.name
      memory_limiter:
        check_interval: 2s
        limit_mib: ${SPLUNK_MEMORY_LIMIT_MIB}
      resource:
        attributes:
          - action: insert
            key: k8s.node.name
            value: ${K8S_NODE_NAME}
          - action: upsert
            key: k8s.cluster.name
            value: test-cluster
      resource/add_agent_k8s:
        attributes:
          - action: insert
            key: k8s.pod.name
            value: ${K8S_POD_NAME}
          - action: insert
            key: k8s.pod.uid
            value: ${K8S_POD_UID}
          - action: insert
            key: k8s.namespace.name
            value: ${K8S_NAMESPACE}
      resource/logs:
        attributes:
          - action: upsert
            from_attribute: k8s.pod.annotations.splunk.com/sourcetype
            key: com.splunk.sourcetype
          - action: delete
            key: k8s.pod.annotations.splunk.com/sourcetype
          - action: delete
            key: splunk.com/exclude
      resourcedetection:
        detectors:
          - env
          - system
        override: true
        timeout: 10s
    receivers:
      filelog:
        encoding: utf-8
        exclude:
          - /var/log/pods/_test-default*_*/otel-collector/*.log
        fingerprint_size: 1kb
        force_flush_period: "0"
        include:
          - /var/log/pods/*/*/*.log
        include_file_name: false
        include_file_path: true
        max_concurrent_files: 1024
        max_log_size: 1MiB
        operators:
          - id: get-format
            routes:
              - expr: body matches "^\\{"
                output: parser-docker
              - expr: body matches "^[^ Z]+ "
                output: parser-crio
              - expr: body matches "^[^ Z]+Z"
                output: parser-containerd
            type: router
          - id: parser-crio
            regex: ^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$
            timestamp:
              layout: "2006-01-02T15:04:05.999999999-07:00"
              layout_type: gotime
              parse_from: attributes.time
            type: regex_parser
          - combine_field: attributes.log
            combine_with: ""
            id: crio-recombine
            is_last_entry: attributes.logtag == 'F'
            output: handle_empty_log
            source_identifier: attributes["log.file.path"]
            type: recombine
          - id: parser-containerd
            regex: ^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$
            timestamp:
              layout: '%Y-%m-%dT%H:%M:%S.%LZ'
              parse_from: attributes.time
            type: regex_parser
          - combine_field: attributes.log
            combine_with: ""
            id: containerd-recombine
            is_last_entry: attributes.logtag == 'F'
            output: handle_empty_log
            source_identifier: attributes["log.file.path"]
            type: recombine
          - id: parser-docker
            output: handle_empty_log
            timestamp:
              layout: '%Y-%m-%dT%H:%M:%S.%LZ'
              parse_from: attributes.time
            type: json_parser
          - combine_field: attributes.log
            combine_with: ""
            id: docker-recombine
            is_last_entry: attributes.log endsWith "\n"
            output: handle_empty_log
            source_identifier: attributes["log.file.path"]
            type: recombine
          - field: attributes.log
            id: handle_empty_log
            if: attributes.log == nil
            type: add
            value: ""
          - parse_from: attributes["log.file.path"]
            regex: ^\/var\/log\/pods\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[^\/]+)\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$
            type: regex_parser
          - from: attributes.uid
            to: resource["k8s.pod.uid"]
            type: move
          - from: attributes.restart_count
            to: resource["k8s.container.restart_count"]
            type: move
          - from: attributes.container_name
            to: resource["k8s.container.name"]
            type: move
          - from: attributes.namespace
            to: resource["k8s.namespace.name"]
            type: move
          - from: attributes.pod_name
            to: resource["k8s.pod.name"]
            type: move
          - field: resource["com.splunk.sourcetype"]
            type: add
            value: EXPR("kube:container:"+resource["k8s.container.name"])
          - from: attributes.stream
            to: attributes["log.iostream"]
            type: move
          - from: attributes["log.file.path"]
            to: resource["com.splunk.source"]
            type: move
          - from: attributes.log
            id: clean-up-log-record
            to: body
            type: move
        poll_interval: 200ms
        start_at: beginning
      fluentforward:
        endpoint: 0.0.0.0:8006
      hostmetrics:
        collection_interval: 10s
        scrapers:
          cpu: null
          disk: null
          filesystem: null
          load: null
          memory: null
          network: null
          paging: null
          processes: null
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268
      kubeletstats:
        auth_type: serviceAccount
        collection_interval: 10s
        endpoint: ${K8S_NODE_IP}:10250
        extra_metadata_labels:
          - container.id
        metric_groups:
          - container
          - pod
          - node
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      prometheus/agent:
        config:
          scrape_configs:
            - job_name: otel-agent
              scrape_interval: 10s
              static_configs:
                - targets:
                    - ${K8S_POD_IP}:8889
      receiver_creator:
        receivers:
          smartagent/coredns:
            config:
              extraDimensions:
                metric_source: k8s-coredns
              port: 9153
              type: coredns
            rule: type == "pod" && labels["k8s-app"] == "kube-dns"
          smartagent/kube-controller-manager:
            config:
              extraDimensions:
                metric_source: kubernetes-controller-manager
              port: 10257
              skipVerify: true
              type: kube-controller-manager
              useHTTPS: true
              useServiceAccount: true
            rule: type == "pod" && labels["k8s-app"] == "kube-controller-manager"
          smartagent/kubernetes-apiserver:
            config:
              extraDimensions:
                metric_source: kubernetes-apiserver
              skipVerify: true
              type: kubernetes-apiserver
              useHTTPS: true
              useServiceAccount: true
            rule: type == "port" && port == 443 && pod.labels["k8s-app"] == "kube-apiserver"
          smartagent/kubernetes-proxy:
            config:
              extraDimensions:
                metric_source: kubernetes-proxy
              port: 10249
              type: kubernetes-proxy
            rule: type == "pod" && labels["k8s-app"] == "kube-proxy"
          smartagent/kubernetes-scheduler:
            config:
              extraDimensions:
                metric_source: kubernetes-scheduler
              port: 10251
              type: kubernetes-scheduler
            rule: type == "pod" && labels["k8s-app"] == "kube-scheduler"
        watch_observers:
          - k8s_observer
      signalfx:
        endpoint: 0.0.0.0:9943
      smartagent/signalfx-forwarder:
        listenAddress: 0.0.0.0:9080
        type: signalfx-forwarder
      zipkin:
        endpoint: 0.0.0.0:9411
    service:
      extensions:
        - file_storage
        - health_check
        - k8s_observer
        - memory_ballast
        - zpages
      pipelines:
        logs:
          exporters:
            - splunk_hec/o11y
          processors:
            - memory_limiter
            - k8sattributes
            - filter/logs
            - batch
            - resource/logs
            - resourcedetection
            - resource
          receivers:
            - filelog
            - fluentforward
            - otlp
        metrics:
          exporters:
            - signalfx
          processors:
            - memory_limiter
            - batch
            - resourcedetection
            - resource
          receivers:
            - hostmetrics
            - kubeletstats
            - otlp
            - receiver_creator
            - signalfx
        metrics/agent:
          exporters:
            - signalfx
          processors:
            - memory_limiter
            - batch
            - resource/add_agent_k8s
            - resourcedetection
            - resource
          receivers:
            - prometheus/agent
        traces:
          exporters:
            - sapm
            - signalfx
          processors:
            - memory_limiter
            - k8sattributes
            - batch
            - resourcedetection
            - resource
          receivers:
            - otlp
            - jaeger
            - smartagent/signalfx-forwarder
            - zipkin
      telemetry:
        metrics:
          address: 0.0.0.0:8889
---
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: splunk-otel-collector
    app.kubernetes.io/managed-by: splunk-otel-collector-operator
    app.kubernetes.io/name: test-default-cluster-receiver
    app.kubernetes.io/part-of: opentelemetry
  name: test-default-cluster-receiver
  namespace: kuttl-test-magical-man
data:
  relay.yaml: |
    exporters:
      signalfx:
        access_token: ${SPLUNK_OBSERVABILITY_ACCESS_TOKEN}
        api_url: https://api.${SPLUNK_REALM}.signalfx.com
        ingest_url: https://ingest.${SPLUNK_REALM}.signalfx.com
        timeout: 10s
    extensions:
      health_check: null
      memory_ballast:
        size_mib: ${SPLUNK_BALLAST_SIZE_MIB}
    processors:
      batch: null
      memory_limiter:
        check_interval: 2s
        limit_mib: ${SPLUNK_MEMORY_LIMIT_MIB}
      resource:
        attributes:
          - action: insert
            key: metric_source
            value: kubernetes
          - action: upsert
            key: k8s.cluster.name
            value: test-cluster
      resource/add_collector_k8s:
        attributes:
          - action: insert
            key: k8s.node.name
            value: ${K8S_NODE_NAME}
          - action: insert
            key: k8s.pod.name
            value: ${K8S_POD_NAME}
          - action: insert
            key: k8s.pod.uid
            value: ${K8S_POD_UID}
          - action: insert
            key: k8s.namespace.name
            value: ${K8S_NAMESPACE}
      resource/k8s_cluster:
        attributes:
          - action: insert
            key: receiver
            value: k8scluster
      resourcedetection:
        detectors:
          - env
          - system
        override: true
        timeout: 10s
    receivers:
      k8s_cluster:
        auth_type: serviceAccount
        metadata_exporters:
          - signalfx
      prometheus/k8s_cluster_receiver:
        config:
          scrape_configs:
            - job_name: otel-k8s-cluster-receiver
              scrape_interval: 10s
              static_configs:
                - targets:
                    - ${K8S_POD_IP}:8889
    service:
      extensions:
        - health_check
        - memory_ballast
      pipelines:
        metrics:
          exporters:
            - signalfx
          processors:
            - memory_limiter
            - batch
            - resource
            - resource/k8s_cluster
          receivers:
            - k8s_cluster
        metrics/collector:
          exporters:
            - signalfx
          processors:
            - memory_limiter
            - batch
            - resource/add_collector_k8s
            - resourcedetection
            - resource
          receivers:
            - prometheus/k8s_cluster_receiver
      telemetry:
        metrics:
          address: 0.0.0.0:8889
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "8888"
    prometheus.io/scrape: "true"
  labels:
    app.kubernetes.io/component: splunk-otel-collector
    app.kubernetes.io/managed-by: splunk-otel-collector-operator
    app.kubernetes.io/name: test-default-cluster-receiver
    app.kubernetes.io/part-of: opentelemetry
  name: test-default-cluster-receiver
  ownerReferences:
    - apiVersion: otel.splunk.com/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: Agent
      name: test-default
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: splunk-otel-collector
      app.kubernetes.io/managed-by: splunk-otel-collector-operator
      app.kubernetes.io/name: test-default-cluster-receiver
      app.kubernetes.io/part-of: opentelemetry
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/component: splunk-otel-collector
        app.kubernetes.io/managed-by: splunk-otel-collector-operator
        app.kubernetes.io/name: test-default-cluster-receiver
        app.kubernetes.io/part-of: opentelemetry
    spec:
      containers:
        - args:
            - --config=/conf/relay.yaml
          env:
            - name: SPLUNK_MEMORY_TOTAL_MIB
              value: "500"
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: K8S_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: K8S_POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: K8S_POD_UID
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.uid
            - name: K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: SPLUNK_OBSERVABILITY_ACCESS_TOKEN
              valueFrom:
                secretKeyRef:
                  key: splunk_observability_access_token
                  name: splunk-access-token
          imagePullPolicy: IfNotPresent
          name: otc-container
          resources:
            limits:
              cpu: 200m
              memory: 500Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
            - mountPath: /conf
              name: collector-configmap
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: splunk-otel-operator-account
      serviceAccountName: splunk-otel-operator-account
      terminationGracePeriodSeconds: 30
      volumes:
        - configMap:
            defaultMode: 420
            items:
              - key: relay.yaml
                path: relay.yaml
            name: test-default-cluster-receiver
          name: collector-configmap
status:
  availableReplicas: 1
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: test-default-agent
  annotations:
    deprecated.daemonset.template.generation: "1"
    prometheus.io/path: /metrics
    prometheus.io/port: "8888"
    prometheus.io/scrape: "true"
  labels:
    app.kubernetes.io/component: splunk-otel-collector
    app.kubernetes.io/managed-by: splunk-otel-collector-operator
    app.kubernetes.io/name: test-default-agent
    app.kubernetes.io/part-of: opentelemetry
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/component: splunk-otel-collector
        app.kubernetes.io/managed-by: splunk-otel-collector-operator
        app.kubernetes.io/name: test-default-agent
        app.kubernetes.io/part-of: opentelemetry
    spec:
      containers:
      - args:
        - --config=/conf/relay.yaml
        env:
        - name: SPLUNK_OBSERVABILITY_ACCESS_TOKEN
          valueFrom:
            secretKeyRef:
              key: access-token
              name: splunk-access-token
        - name: SPLUNK_REALM
          value: my-splunk-realm
        - name: MY_CLUSTER_NAME
          value: test-cluster
        - name: HOST_PROC
          value: /hostfs/proc
        - name: HOST_SYS
          value: /hostfs/sys
        - name: HOST_ETC
          value: /hostfs/etc
        - name: HOST_VAR
          value: /hostfs/var
        - name: HOST_RUN
          value: /hostfs/run
        - name: HOST_DEV
          value: /hostfs/dev
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: MY_NODE_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: MY_POD_UID
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.uid
        - name: MY_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: SPLUNK_MEMORY_TOTAL_MIB
          value: "500"
        image: quay.io/signalfx/splunk-otel-collector:0.61.0
        imagePullPolicy: IfNotPresent
        name: otc-container
        resources:
          limits:
            cpu: 200m
            memory: 500Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /conf
          name: collector-configmap
        - mountPath: /hostfs
          mountPropagation: HostToContainer
          name: hostfs
          readOnly: true
        - mountPath: /etc/passwd
          name: etc-passwd
          readOnly: true
      dnsPolicy: ClusterFirst
      hostNetwork: true
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: splunk-otel-operator-account
      serviceAccountName: splunk-otel-operator-account
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoSchedule
        key: node.alpha.kubernetes.io/role
        operator: Exists
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
        operator: Exists
      volumes:
      - configMap:
          defaultMode: 420
          items:
          - key: relay.yaml
            path: relay.yaml
          name: test-default-agent
        name: collector-configmap
      - hostPath:
          path: /
          type: ""
        name: hostfs
      - hostPath:
          path: /etc/passwd
          type: ""
        name: etc-passwd